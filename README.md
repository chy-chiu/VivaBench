# VivaBench: Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in LLMs

This repository is the official implementation of *VivaBenchâ€”â€œSimulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models.â€*

VivaBench is a multi-turn benchmark of 1,152 physician-curated clinical vignettes that simulates a viva voce (oral) exam: agents must iteratively gather H&P findings and order investigations to arrive at a diagnosis.

## ğŸ“‹ Requirements

API-keys for OpenAI/OpenRouter if you use those providers. See **Configuration** below.

## ğŸ›  Installation
Install the package in editable mode to expose the vivabench console script:

```
git clone 
pip install -e .
```

```
$ which vivabench
/path/to/venv/bin/vivabench
```
## âš™ï¸ Configuration

All pipeline parameters live in YAML:

- **configs/evaluate.yaml**
    
    - `data.input`Â â†’ input CSV of vignettes
    - `data.output_dir`Â â†’ where to write logs & results
    - `data.batch_size`,Â `data.max_workers`
    - `models.examiner`,Â `models.agent`Â blocks (provider, model, temp, APIâ€key/env)
    - `examination.*`Â â†’ mapper/parser limits & SNOMED path
    - `logging.level`
- **configs/generate.yaml**
    
    - `pipeline.input`Â /Â `pipeline.output`Â /Â `pipeline.batch_size`Â /Â `pipeline.limit`
    - `embeddings.*`,Â `mappings.*`
    - `models.generator`,Â `models.reasoning`
    - `logging.level`

Edit the defaults, or override via CLI flags.

---

## ğŸ“š Demo
To get an overview of the core functions within the VivaBench framework, the best entry point is `demo.ipynb`. 


## ğŸš€ CLI Usage

### 1. Run the Evaluation Pipeline
To reproduce experiment results outlined in our paper, simply run the evaluation pipeline

```bash
vivabench evaluate \
  --config configs/evaluate.yaml \
  [--input     /path/to/my_input.csv] \
  [--output_dir /path/to/outdir] \
  [--evaluation_id id_of_evaluation_run]
```

- ReadsÂ `data.input`Â orÂ `--input`Â override
- Instantiates examiner & agent models viaÂ `init_chat_model`,Â `init_openrouter_chat_model`, orÂ `init_ollama_chat_model`
- ExecutesÂ `run_examinations_parallel(...)`
- Saves per-case logs inÂ `output_dir/logs/`Â and results CSVs inÂ `output_dir/results/`

### 2. Re-run Metrics on Existing Output
The evaluation pipeline runs metrics by default. However, if you want to re-run metrics on a specific file, you can use this command

```bash
vivabench metrics \
  --config configs/evaluate.yaml \
  --output_csv /path/to/results/full_results.csv \
  [--output_dir /path/to/metrics_out]
```

- Loads your evaluation YAML & theÂ `--output_csv`
- CallsÂ `EvaluationMetrics(...)`Â to compute accuracy, precision/recall, confidence scores
- WritesÂ `metrics.csv`Â under the same output directory

### 3. Run the Generation Pipeline
If you want to generate more cases from clinical vignettes, you can use this command
```bash
vivabench generate \
  --config configs/generate.yaml \
  [--input  /path/to/seed_vignettes.csv] \
  [--output /path/to/generated.csv]
```

- Builds and runsÂ `PipelineConfig(...)`Â â†’Â `run_pipeline(...)`
- Produces a structured clinical case dataset in the specifiedÂ `pipeline.output`

## ğŸ“ Citation
If you use VivaBench in your work, please cite:


```
@article{vivabench2025,
  title   = {Simulating Viva Voce Examinations to Evaluate Clinical Reasoning in Large Language Models},
  author  = {Anonymous Author(s)},
  journal = {},
  year    = {2025},
}
```
## ğŸ“ License & Contributing
This project is released under the CC-NA License. Contributions welcomeâ€”please open an issue or pull request.